Feature Representation for the large-scale- scene: co-visibility map
reference: https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Efficient_Global_2D-3D_ICCV_2017_paper.pdf

Texture mapping:
https://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Texture_Mapping_for_CVPR_2018_paper.pdf

RGB registration --> find intersection of the ray with the mesh --> ray tracing
referemce: Octree-R: An Adaptive Octree for Efficient Ray Tracing
https://stackoverflow.com/questions/66968822/intersection-between-2d-image-point-and-3d-mesh

Advice from Petr: looking at the neural network: how to put the scene feature for better gaze estimation. Reference: Adria Recasens gaze following


ESCNet: Gaze Target Detection with the Understanding of 3D Scenes
- only uses the 2D visual cues as input
- first estimate a rough 3D point cloud with images and estimated depth -> extract the front-most map as scene contextual representation


'Where are they looking' from Adria Recasens:
- Using a network to predict the gaze pathway and sailency pathway
- Inputs are original image and image of the head