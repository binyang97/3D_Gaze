Feature Representation for the large-scale- scene: co-visibility map
reference: https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Efficient_Global_2D-3D_ICCV_2017_paper.pdf

Texture mapping:
https://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Texture_Mapping_for_CVPR_2018_paper.pdf

RGB registration --> find intersection of the ray with the mesh --> ray tracing
referemce: Octree-R: An Adaptive Octree for Efficient Ray Tracing
https://stackoverflow.com/questions/66968822/intersection-between-2d-image-point-and-3d-mesh

Advice from Petr: looking at the neural network: how to put the scene feature for better gaze estimation. Reference: Adria Recasens gaze following


ESCNet: Gaze Target Detection with the Understanding of 3D Scenes
- only uses the 2D visual cues as input
- first estimate a rough 3D point cloud with images and estimated depth -> extract the front-most map as scene contextual representation


'Where are they looking' from Adria Recasens:
- Using a network to predict the gaze pathway and sailency pathway
- Inputs are original image and image of the head

--> The hint from Adira Recasens: 
The saliency map of an image represents the most prominent and focused pixel of an image. Sometimes, the brighter pixels of an image tell us about the salient of the pixels. This means the brightness of the pixel is directly proportional to the saliency of an image.
Extract Sailency Map from image --> 3D Sailency Map from scene --> New feature for gaze estimation


