{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84633162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from sys import platform\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from pupil_apriltags import Detector\n",
    "import open3d as o3d\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import collections\n",
    "import k3d\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ead08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def position_error(T_e, T_gt):\n",
    "\n",
    "    return np.linalg.norm(T_e- T_gt)\n",
    "\n",
    "\n",
    "def rotation_error(R_e, R_gt):\n",
    "    \n",
    "    return (180/math.pi) * np.arccos((np.trace(np.matmul(R_e, R_gt.T))-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ac1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k3d_frustrum(pose, name, size=0.009, color=0x0000ff):\n",
    "    # i.e. not using image sizes \n",
    "    pos = pose[:3, 3]\n",
    "    \n",
    "    forward = -pose[:3, 2] * size * -1.4\n",
    "    right = -pose[:3, 0] * size * 1.25\n",
    "    up = -pose[:3, 1] * size\n",
    "    \n",
    "    #verts = [pos, pos + forward*size ]\n",
    "    verts = [pos, pos + forward - right*0.5 + up*0.5, pos + forward + right * 0.5 + up * 0.5, pos ]\n",
    "    verts += [pos, pos + forward - right*0.5 - up*0.5, pos + forward + right * 0.5 - up * 0.5, pos ]\n",
    "    verts += [pos, pos + forward - right*0.5 + up*0.5, pos + forward - right * 0.5 - up * 0.5, pos]\n",
    "    verts += [pos, pos + forward + right*0.5 + up*0.5, pos + forward + right * 0.5 - up * 0.5, pos]\n",
    "    return k3d.line(verts, color=color, width=2.5, name = name, shader=\"simple\")\n",
    "\n",
    "def visualization_registration(reg, mesh):\n",
    "    vertices = [ ]\n",
    "\n",
    "    frustrums = []\n",
    "\n",
    "    colors = [0xff0000, 0x00ff00, 0x0000ff, 0xffcc00, 0xccff00, 0x00ccff]\n",
    "\n",
    "    for i, frame in enumerate(reg.keys()):\n",
    "        pos = reg[frame][:3, 3] \n",
    "        vertices += pos.tolist()\n",
    "        frustrums.append( k3d_frustrum(reg[frame], name = str(frame), size=0.1, color=colors[i % len(colors)]) )\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "\n",
    "    lines = k3d.line(vertices, color=0xff0000, width=2.5, shader=\"simple\") # + line\n",
    "    pts = k3d.points(vertices, point_size=0.003)\n",
    "\n",
    "    plot3d = k3d.plot(antialias=True, camera_auto_fit=True)\n",
    "\n",
    "    #plot3d += lines\n",
    "    plot3d += pts \n",
    "\n",
    "    for f in frustrums:\n",
    "        plot3d += f\n",
    "\n",
    "    plot3d += k3d.points( [vertices[:3]], point_size=0.0075, color=0x00ff00)\n",
    "\n",
    "    plot3d += k3d.mesh(np.array(mesh.vertices), np.array(mesh.triangles).flatten(), color=0xffcc00)\n",
    "    plot3d.display()\n",
    "\n",
    "def linear_fit(x, y, err = \"Mean\"):\n",
    "    \"\"\"For set of points `(xi, yi)`, return linear polynomial `f(x) = k*x + m` that\n",
    "    minimizes the sum of quadratic errors.\n",
    "    \"\"\"\n",
    "    meanx = sum(x) / len(x)\n",
    "    meany = sum(y) / len(y)\n",
    "    k = sum((xi-meanx)*(yi-meany) for xi,yi in zip(x,y)) / sum((xi-meanx)**2 for xi in x)\n",
    "    m = meany - k*meanx\n",
    "\n",
    "    sum_error = 0\n",
    "    for xi, yi in zip(x,y):\n",
    "        y_line = k*xi + m\n",
    "        sum_error += abs(y_line - yi)\n",
    "    \n",
    "    if err == \"Mean\":\n",
    "        error = sum_error / len(x)\n",
    "    elif err == \"Sum\":\n",
    "        error = sum_error\n",
    "\n",
    "    return k, m, error\n",
    "\n",
    "at_detector = Detector(\n",
    "            families=\"tagStandard41h12\",\n",
    "            nthreads=1,\n",
    "            quad_decimate=1.0,\n",
    "            quad_sigma=0.0,\n",
    "            refine_edges=1,\n",
    "            decode_sharpening=0.25,\n",
    "            debug=0,\n",
    "        )\n",
    "\n",
    "\n",
    "Register = collections.namedtuple(\n",
    "    \"RegisterInfo\", [\"CameraPose\", \"Intrinsic\", \"TagPose\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b08d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "if platform == \"linux\" or platform == \"linux2\":  \n",
    "    # linux\n",
    "    data_path  = \"/home/biyang/Documents/3D_Gaze/dataset/3D_scanner_app/Apriltag1_dataset1\"\n",
    "    data_pi_path = \"/home/biyang/Documents/3D_Gaze/dataset/PupilInvisible/office1/data_1\"\n",
    "elif platform == \"win32\":\n",
    "# Windows...\n",
    "    data_path = r\"D:\\Documents\\Semester_Project\\3D_Gaze\\dataset\\3D_Scanner_App\\Apriltag2-dataset2\"\n",
    "    data_pi_path = r\"D:\\Documents\\Semester_Project\\3D_Gaze\\dataset\\PupilInvisible\\office1\\data_1\"\n",
    "\n",
    "# Getting the Visualization\n",
    "VISUALIZATION = False\n",
    "TAG_POSE_VISUALIZATION = False\n",
    "DATA = \"IPHONE\" # \"PI\" or \"IPHONE\"\n",
    "\n",
    "Evaluation = {}\n",
    "\n",
    "at_detector = Detector(\n",
    "        families=\"tagStandard41h12\",\n",
    "        nthreads=1,\n",
    "        quad_decimate=1.0,\n",
    "        quad_sigma=0.0,\n",
    "        refine_edges=1,\n",
    "        decode_sharpening=0.25,\n",
    "        debug=0,\n",
    "    )\n",
    "\n",
    "if DATA == \"IPHONE\":\n",
    "\n",
    "    images_path = os.path.join(data_path, \"frames\")\n",
    "    pose_path = os.path.join(data_path, \"pose\")\n",
    "    mesh_fullpath = os.path.join(data_path, \"data3d/textured_output.obj\")\n",
    "    depth_path = os.path.join(data_path, \"depth\")\n",
    "\n",
    "elif DATA == \"PI\":\n",
    "    data_path = data_pi_path\n",
    "    images_path = os.path.join(data_path, \"images_undistorted\")\n",
    "\n",
    "tag_sizes = np.arange(0.05, 0.11, 0.01)\n",
    "real_tag_size = 0.087\n",
    "\n",
    "\n",
    "images_files = os.listdir(images_path)\n",
    "\n",
    "images_files.sort()\n",
    "\n",
    "tag_points_3d = []\n",
    "\n",
    "Vis_frames = []\n",
    "if DATA == \"IPHONE\":\n",
    "\n",
    "    img_width = 1440\n",
    "    img_height = 1920\n",
    "\n",
    "elif DATA == \"PI\":\n",
    "    img_width = 1088\n",
    "    img_height = 1080\n",
    "    print(\"There is no extrinsic matrix and 3d model for data recorded by PI, so there is no 3d visualization, only visulization with tag pose\")\n",
    "\n",
    "for i, image_file in enumerate(images_files):\n",
    "    # if i%5 != 0:\n",
    "    #     continue\n",
    "    \n",
    "\n",
    "    img = cv2.imread(os.path.join(images_path, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "    color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    if DATA == \"IPHONE\":\n",
    "    \n",
    "        camera_param_file = image_file.replace(\".jpg\", \".json\")\n",
    "        with open(os.path.join(pose_path, camera_param_file), 'r') as f:\n",
    "            camera_param = json.load(f)\n",
    "\n",
    "        intrinsics = np.array(camera_param[\"intrinsics\"]).reshape(3, 3)\n",
    "\n",
    "        projectionMatrix = np.array(camera_param[\"projectionMatrix\"]).reshape(4, 4)\n",
    "\n",
    "    elif DATA == \"PI\":\n",
    "        intrinsics = np.array([[766.2927454396544, 0.0, 543.6272327745995],\n",
    "                            [0.0, 766.3976103393867, 566.0580149497666],\n",
    "                            [0.0, 0.0, 1.0]])\n",
    "\n",
    "        projectionMatrix = np.eye(4)\n",
    "        \n",
    "    fxfycxcy= [intrinsics[0, 0], intrinsics[1, 1], intrinsics[0, 2], intrinsics[1, 2]]\n",
    "    principal_point = np.array([intrinsics[0, 2], intrinsics[1, 2]])\n",
    "    focal_length = (intrinsics[0, 0] + intrinsics[1, 1]) / 2\n",
    "\n",
    "    # The real size of the tag is about 8.7 cm\n",
    "    tags_real = at_detector.detect(img, estimate_tag_pose=True, camera_params = fxfycxcy, tag_size=real_tag_size)\n",
    "    if len(tags_real) == 0:\n",
    "        continue\n",
    "    \n",
    "    alpha_real = []\n",
    "    for tag_real in tags_real:\n",
    "        tag_id = tag_real.tag_id\n",
    "        if tag_id not in Evaluation.keys():\n",
    "            Evaluation[tag_id] = {\"Real_Distance\": [],\n",
    "                                    \"Frame_id\": [],\n",
    "                                    \"Error_Stability\": [],\n",
    "                                    \"Error_Accuracy\": [],\n",
    "                                    \"angle_x\": [],\n",
    "                                    \"angle_y\": [],\n",
    "                                    \"angle_z\": []}\n",
    "\n",
    "            # Add the real distance\n",
    "        # Project the line onto the plane which is parallel to the image plane\n",
    "\n",
    "        R_tag2cam = np.array(tag_real.pose_R)\n",
    "        t_tag2cam = np.array(tag_real.pose_t)\n",
    "\n",
    "        r = R.from_matrix(R_tag2cam)\n",
    "        euler_angles = r.as_euler('xyz', degrees=True)\n",
    "\n",
    "        Evaluation[tag_id][\"Real_Distance\"].append(np.linalg.norm(t_tag2cam))\n",
    "        Evaluation[tag_id][\"Frame_id\"].append(image_file)\n",
    "        Evaluation[tag_id][\"angle_x\"].append(euler_angles[0])\n",
    "        Evaluation[tag_id][\"angle_y\"].append(euler_angles[1])\n",
    "        Evaluation[tag_id][\"angle_z\"].append(euler_angles[2])\n",
    "\n",
    "        alpha_real_tag_mean = 0\n",
    "        for i, tag_corner in enumerate(tag_real.corners):\n",
    "            # 0: right bottom, 1: right top, 2: left top, 3: left bottom\n",
    "            tag_center = np.array(tag_real.center)\n",
    "            if i == 0:\n",
    "                tag_corner_tag_coord = np.array([[1], [1], [0]]) * (real_tag_size/2)\n",
    "            elif i == 1:\n",
    "                tag_corner_tag_coord = np.array([[1], [-1], [0]]) * (real_tag_size/2)\n",
    "            elif i == 2:\n",
    "                tag_corner_tag_coord = np.array([[-1], [-1], [0]]) * (real_tag_size/2)\n",
    "            elif i == 3:\n",
    "                tag_corner_tag_coord = np.array([[-1], [1], [0]]) * (real_tag_size/2)\n",
    "            tag_corner_cam_coord = R_tag2cam @ tag_corner_tag_coord + t_tag2cam\n",
    "            tag_center_cam_coord = t_tag2cam\n",
    "\n",
    "            delta_l = math.sqrt(math.pow((tag_corner_cam_coord[0]*tag_center_cam_coord[2]/tag_corner_cam_coord[2] - tag_center_cam_coord[0]), 2) +\n",
    "                                math.pow((tag_corner_cam_coord[1]*tag_center_cam_coord[2]/tag_corner_cam_coord[2] - tag_center_cam_coord[1]), 2))\n",
    "\n",
    "            \n",
    "            delta_d = math.sqrt(math.pow(focal_length, 2) + math.pow(np.linalg.norm(tag_center - principal_point), 2))\n",
    "            delta_uv_tag = np.linalg.norm(tag_corner-tag_center)\n",
    "            alpha_real_tag_mean += (delta_l / real_tag_size) * (delta_d / delta_uv_tag)\n",
    "        alpha_real.append(alpha_real_tag_mean / 4)\n",
    "\n",
    "        \n",
    "    \n",
    "    true_distances = []\n",
    "    for tag_size in tag_sizes:   \n",
    "        \n",
    "        tags = at_detector.detect(img, estimate_tag_pose=True, camera_params = fxfycxcy, tag_size=tag_size)\n",
    "        \n",
    "        # Test with only one tag\n",
    "        for tag_test in tags:\n",
    "            \n",
    "            tag_position = np.array(tag_test.pose_t)\n",
    "            tag_cam_distance = np.linalg.norm(tag_position)\n",
    "            true_distances.append(tag_cam_distance)\n",
    "\n",
    "    true_distances = np.array(true_distances).reshape(-1, len(tags))\n",
    "    print(true_distances.shape)\n",
    "    for i, tag in enumerate(tags_real):\n",
    "        alpha_test, _, err = linear_fit(tag_sizes, true_distances[:,i])\n",
    "        alpha_error = abs(alpha_real[i] - alpha_test)\n",
    "        \n",
    "\n",
    "\n",
    "        Evaluation[tag.tag_id][\"Error_Stability\"].append(err)\n",
    "        Evaluation[tag.tag_id][\"Error_Accuracy\"].append(alpha_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "898bf8c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if platform == \"linux\" or platform == \"linux2\":  \n",
    "# # linux\n",
    "#     data_path  = \"/home/biyang/Documents/3D_Gaze/dataset/3D_scanner_app/Apriltag1_dataset1\"\n",
    "#     data_pi_path = \"/home/biyang/Documents/3D_Gaze/dataset/PupilInvisible/office1/data_1\"\n",
    "#     evaluation_json_path = r\"/home/biyang/Documents/3D_Gaze/dataset/PupilInvisible/evaluation_apriltag_detection_Iphone.json\"\n",
    "# elif platform == \"win32\":\n",
    "# # Windows...\n",
    "#     data_path = r\"D:\\Documents\\Semester_Project\\3D_Gaze\\dataset\\3D_Scanner_App\\Apriltag1_dataset1\"\n",
    "#     data_pi_path = r\"D:\\Documents\\Semester_Project\\3D_Gaze\\dataset\\PupilInvisible\\office1\\data_1\"\n",
    "#     evaluation_json_path = r\"D:\\Documents\\Semester_Project\\3D_Gaze\\dataset\\evaluation_apriltag_detection_Iphone_a1d1.json\"\n",
    "\n",
    "# with open(evaluation_json_path, \"r\") as f:\n",
    "#     evaluation  = json.load(f)\n",
    "\n",
    "\n",
    "evaluation = Evaluation\n",
    "r = R.from_euler('xyz', [0, 180, -90], degrees=True)\n",
    "Additional_Rotation = r.as_matrix()\n",
    "additional_rotation = np.concatenate(\n",
    "                     [np.concatenate([Additional_Rotation, np.array([[0], [0], [0]])], axis=1), np.array([[0, 0, 0, 1]])], axis=0)\n",
    "\n",
    "# Select one frame from iphone for each tag as register\n",
    "track_frame = {}\n",
    "for tag_id in evaluation.keys():\n",
    "    distance_error = [alpha_error * 0.087 for alpha_error in evaluation[tag_id][\"Error_Accuracy\"]]\n",
    "    distance_error = np.array(distance_error)\n",
    "\n",
    "    index = np.argmin(distance_error)\n",
    "    frame_id = evaluation[tag_id][\"Frame_id\"][index]\n",
    "\n",
    "    track_frame[tag_id] = frame_id\n",
    "\n",
    "iphone_frames_folder= os.path.join(data_path, \"frames\")\n",
    "iphone_pose_folder = os.path.join(data_path, \"pose\")\n",
    "mesh_fullpath = os.path.join(data_path, \"data3d/textured_output.obj\")\n",
    "\n",
    "# Extract the camera pose and tag pose of each register\n",
    "register = {}\n",
    "\n",
    "for tag_id, frame_id in track_frame.items():\n",
    "\n",
    "    camera_param_file = frame_id.replace(\".jpg\", \".json\")\n",
    "    img = cv2.imread(os.path.join(iphone_frames_folder, frame_id), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_height, img_width = img.shape\n",
    "    with open(os.path.join(iphone_pose_folder, camera_param_file), 'r') as f:\n",
    "        camera_param = json.load(f)\n",
    "\n",
    "    intrinsics = np.array(camera_param[\"intrinsics\"]).reshape(3, 3)\n",
    "    Cam2World = np.array(camera_param[\"cameraPoseARFrame\"]).reshape(4, 4)\n",
    "\n",
    "    fxfycxcy= [intrinsics[0, 0], intrinsics[1, 1], intrinsics[0, 2], intrinsics[1, 2]]\n",
    "    tags = at_detector.detect(img, estimate_tag_pose=True, camera_params = fxfycxcy, tag_size=0.087)\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag.tag_id == int(tag_id):\n",
    "            tag_pose = np.concatenate(\n",
    "                    [np.concatenate([np.array(tag.pose_R), np.array(tag.pose_t)], axis=1), np.array([[0, 0, 0, 1]])], axis=0)\n",
    "            register[tag.tag_id] = Register(CameraPose=Cam2World, Intrinsic=intrinsics, TagPose=tag_pose)\n",
    "\n",
    "\n",
    "# Compute the Registration \n",
    "# 1. Detect the tag in the frame\n",
    "# 2. Find the register with common tag\n",
    "# 3. Transform the new frame to the world coordinate\n",
    "\n",
    "PI_registration = {}\n",
    "PI_images_folder = os.path.join(data_pi_path, \"images_undistorted\")\n",
    "PI_intrinsics = np.array([[766.2927454396544, 0.0, 543.6272327745995],\n",
    "                            [0.0, 766.3976103393867, 566.0580149497666],\n",
    "                            [0.0, 0.0, 1.0]])\n",
    "PI_fxfycxcy= [PI_intrinsics[0, 0], PI_intrinsics[1, 1], PI_intrinsics[0, 2], PI_intrinsics[1, 2]]\n",
    "images = os.listdir(PI_images_folder)\n",
    "images.sort()\n",
    "\n",
    "for i, img_id in enumerate(images):\n",
    "    if i % 20 != 0:\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(os.path.join(PI_images_folder, img_id), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    PI_tags = at_detector.detect(img, estimate_tag_pose=True, camera_params = PI_fxfycxcy, tag_size=0.087)\n",
    "    if len(PI_tags) == 0:\n",
    "        continue\n",
    "\n",
    "    # Here I select simply one tag for the test\n",
    "    \n",
    "    PI_tag = PI_tags[0]\n",
    "    Link_Register = register[PI_tag.tag_id]\n",
    "    PI_tag_pose = np.concatenate(\n",
    "                    [np.concatenate([np.array(PI_tag.pose_R), np.array(PI_tag.pose_t)], axis=1), np.array([[0, 0, 0, 1]])], axis=0)\n",
    "    # PI_Cam2World\n",
    "    PI_registration[int(img_id.split(\".\")[0])] =  Link_Register.CameraPose \\\n",
    "                                                @ additional_rotation @ Link_Register.TagPose @ np.linalg.inv(PI_tag_pose)\n",
    "\n",
    "\n",
    "#print(PI_registration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "605973af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "IPhone_Registration = {}\n",
    "GT_CameraPose = {}\n",
    "\n",
    "for frame in os.listdir(iphone_frames_folder):\n",
    "    if frame in track_frame.values():\n",
    "        print(\"Passed\")\n",
    "        continue\n",
    "    camera_param_file = frame.replace(\".jpg\", \".json\")\n",
    "    img = cv2.imread(os.path.join(iphone_frames_folder, frame), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img_height, img_width = img.shape\n",
    "    with open(os.path.join(iphone_pose_folder, camera_param_file), 'r') as f:\n",
    "        camera_param = json.load(f)\n",
    "\n",
    "    intrinsics = np.array(camera_param[\"intrinsics\"]).reshape(3, 3)\n",
    "    Cam2World = np.array(camera_param[\"cameraPoseARFrame\"]).reshape(4, 4)\n",
    "\n",
    "    fxfycxcy= [intrinsics[0, 0], intrinsics[1, 1], intrinsics[0, 2], intrinsics[1, 2]]\n",
    "    Iphone_tags = at_detector.detect(img, estimate_tag_pose=True, camera_params = fxfycxcy, tag_size=0.087)\n",
    "    \n",
    "    if len(Iphone_tags) == 0:\n",
    "        continue\n",
    "    Iphone_tag = Iphone_tags[0]\n",
    "    \n",
    "    \n",
    "    Link_Register = register[Iphone_tag.tag_id]\n",
    "    Iphone_tag_pose = np.concatenate(\n",
    "                    [np.concatenate([np.array(Iphone_tag.pose_R), np.array(Iphone_tag.pose_t)], axis=1), np.array([[0, 0, 0, 1]])], axis=0)\n",
    "    # PI_Cam2World\n",
    "    IPhone_Registration[int(frame[6:11])] =  Link_Register.CameraPose @ additional_rotation \\\n",
    "                                                @ Link_Register.TagPose @ np.linalg.inv(Iphone_tag_pose)\n",
    "    \n",
    "    GT_CameraPose[int(frame[6:11])] = Cam2World @ additional_rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9aaddf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_error = []\n",
    "rot_error = []\n",
    "for gt_pose, est_pose in zip(GT_CameraPose.values(), IPhone_Registration.values()):\n",
    "    pos_error.append(position_error(est_pose[:3, 3], gt_pose[:3, 3]))\n",
    "    rot_error.append(rotation_error(est_pose[:3, :3], gt_pose[:3, :3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d607696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39287156656092764, 0.3576447196433138, 0.3447452524677575, 0.41476281781633795, 0.4245448832701054, 0.4143758798962384, 0.3748275854499087, 0.3733304665619207, 0.30523101098061645, 0.2730367717102395, 0.2933363310489819, 0.31215868841537775, 0.31908159042159556, 0.3450272430888063, 0.3645920352340974, 0.3574949555780043, 0.35048974386602993, 0.40366327941303604, 0.43564823177671125, 0.5010783553986502, 0.08896833341947342, 0.02987395502334521, 0.29994083446350944, 0.22885086002863397, 0.14652527228704673, 0.25609114241719216, 0.45900020709124045, 0.3591048600676438, 0.14337956789310816, 0.10459054223162173, 0.06058064956979416]\n",
      "31\n",
      "0.30757573009971817\n"
     ]
    }
   ],
   "source": [
    "print(pos_error)\n",
    "print(len(pos_error))\n",
    "print(sum(pos_error)/len(pos_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e47c399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.808010550706548, 7.712462253952855, 8.378572535634206, 7.441399037507818, 7.353551813755468, 7.446254094031968, 8.021658175031696, 7.981162071274257, 7.643147555091207, 8.463697415504328, 8.62362686179489, 10.13518643576861, 11.511719352405022, 12.414063211737357, 13.632202819483155, 14.700141923623303, 16.072040085291114, 18.624213602993567, 21.214610878405505, 24.99989088297226, 4.9642533382071425, 1.771884258538931, 13.538560386868948, 10.926577485258656, 6.314228078342123, 10.432779305664454, 11.73177440123924, 9.039601293064969, 4.899378457379457, 4.371209936198388, 2.47964207749788]\n",
      "10.020887115329847\n"
     ]
    }
   ],
   "source": [
    "print(rot_error)\n",
    "\n",
    "print(sum(rot_error)/len(rot_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "167d6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_textured = o3d.io.read_triangle_mesh(mesh_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8ba2c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\ybMas\\anaconda3\\envs\\apriltag\\lib\\site-packages\\traittypes\\traittypes.py:101: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n",
      "d:\\Users\\ybMas\\anaconda3\\envs\\apriltag\\lib\\site-packages\\traittypes\\traittypes.py:101: UserWarning: Given trait value dtype \"int32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  np.dtype(self.dtype).name))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf664837df8245c8a2082d7f2737af61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of Camera Pose\n",
    "#visualization_registration(PI_registration, mesh_textured)\n",
    "visualization_registration(GT_CameraPose, mesh_textured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10adfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Der angeforderte Transformationsvorgang wird nicht unterstützt. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Das Handle ist ungültig. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Das Handle ist ungültig. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Der angeforderte Transformationsvorgang wird nicht unterstützt. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Das Handle ist ungültig. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Der angeforderte Transformationsvorgang wird nicht unterstützt. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Der angeforderte Transformationsvorgang wird nicht unterstützt. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: Der angeforderte Transformationsvorgang wird nicht unterstützt. \n"
     ]
    }
   ],
   "source": [
    "pcd_sample = mesh_textured.sample_points_uniformly(number_of_points=10000)\n",
    "o3d.visualization.draw_geometries([pcd_sample])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8feb97a30f0c0881f8e9b0bc542abf8861c7a1cbabf22b5193b3eb6b79469f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
